% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfauthor={Jake Bowers},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[left=1.25in,right=1.25in,bottom=1in,top=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{2}
\usepackage{fancyhdr,quiver,float}
\fancypagestyle{myfancy}{%
    \fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \fancyfoot[R]{Version of \today --- \thepage}}
\pagestyle{myfancy}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{../../Research-Group-Bibliography/big.bib}

\title{How can coordinated studies enhance evidence-based policy
making?}
\author{Jake Bowers\footnote{University of Illinois
  \href{mailto:jwbowers@illinois.edu}{\nolinkurl{jwbowers@illinois.edu}}.
  Thanks to Jim Kuklinski (University of Illinois), Carrie Cihak (King
  County, WA), James Diossa (The Policy Lab @ Brown), and participants
  in The Policy Lab @ Brown University Providence Talks Replication
  project and meetings.}}
\date{Version of 2021-Sep-26}

\begin{document}
\maketitle

\begin{abstract}

Coordinated randomized trials can help convince researchers that certain kinds
of causal explanations are more or less plausible: helping elaborate a shared
causal model linking interventions to outcomes. Policy makers may not share the
same causal model across jurisdictions or with the scientific community. And
policy makers are apt to prefer disaggregated results from coordinated studies
--- focusing on results in contexts similar to their own --- rather than overall
estimates of average causal effects.  Coordinated studies designed in
collaboration between policy makers and academics could create such shared
causal models and thus enhance the utility of coordination from the perspective
of evidence-based public policy making.
\end{abstract}

As the evidence-based public policy movement increases in strength and
scope, policy experts are eager to use evidence (as scientific insight
and/or causal evaluation) to ensure that money is not wasted,
inequalities addressed and most lives are improved. Advances in the
cumulation of general social science knowledge promise to improve the
scientific consensus --- to create better insights into the causal
mechanisms linking possible policy levers and public welfare outcomes.
Yet, evidence does not turn into policy in any easy or simple way. This
project uses observations of policy-makers involved in creating a
coordinated experiment plus interviews with a two other policy-makers
(so far) to highlight some of the challenges that policy-makers face in
using evidence in the form of causal relationships reported from any
study, coordinated or otherwise. It also recommends two approaches to
coordination that may make it a little easier for evidence from
coordinated studies to add to evidence-based public policy making:
collaborative learning agendas shared across jurisdictions and academics
involving explicit causal models and the participation of non-field site
jurisdictions in the design of coordinated studies.

First, I describe some of the problems that would face a policy-maker
who would like to build and/or justify a new policy intervention based
on results from a single study completed in a different context. Second,
I describe the benefits of coordinated studies in regards answering
critics of a single study. Third, I describe some of the challenges
still facing policy makers in a given place when it comes to creating
and justifying a new intervention even given a coordinated study.
Finally, I defend the idea of coordinated studies as useful for
evidence-based policy making and explain the suggestions made to improve
them for this purpose above.

\hypertarget{the-problem-of-a-single-study-for-evidence-based-policy-making.}{%
\section{The problem of a single study for evidence-based policy
making.}\label{the-problem-of-a-single-study-for-evidence-based-policy-making.}}

A group of policy makers must make a policy and oversee its
implementation in a given place and time.\footnote{I'm imagining a city
  council or other executive group in this paper.} This policy is a
change to some process that the policy makers hope will improve some
outcomes in their jurisdiction. For example, they might hope to improve
both the mental and physical health, comfort, and neighborhood attitudes
of urban neighborhoods by planting trees along the sidewalk. Or they
might hope to improve the kindergarten readiness of low income children
by funding a program to coach families in how to talk and read with
their toddlers. Or they might make all public transit free in their city
in order to reduce inequality and improve the economy. In each case they
have choices about how to pursue these outcomes. And they also face
trade-offs across different areas.

Studies reporting estimated causal effects of coaching on kindergarten
readiness or tree planting on attitudes should, in principle, help
policy makers. Or at least, many researchers justify their research to
themselves and others on the grounds of policy relevance. And, to date,
a fan of evidence-based policy and a proponent of a new approach might
say: ``This is a promising approach that I heard about that worked in
City B.'' and then explain by the listeners should take the result
seriously by saying, ``The finding was statistically significant. I
think it was an RCT.'' And others sharing the same implicit causal model
linking approach to outcome and perhaps same values that orient them to
desire an change in the outcome or use of the intervention might find
this report sufficient to launch a conversation about implementation of
the approach in their city (say, City A).

However, even as people with a priori reasons to be attracted to a given
new approach (shared causal model and values orientation) discuss
implementation problems, they also know that researchers can criticize
the results of any single study on many grounds. If the results do not
agree with the priors or values of some other group, such opponents
might dismiss the results as mere ideology masquerading as science if
the analysis was not done in a transparent way (for example if the
analysis plan was not pre-registered or the code and data were not made
public). Or they might argue that even a \(p < .01\) for a null
hypothesis of no effects merely means that a given result could be a
rare event --- yes, surprising from the perspective of the null
hypothesis, but making an rare error doesn't change the cost of that
error for the government and the public. If the result arose in a study
in a context very different from the given city, critics might argue
that, say, a positive result in City B ought not be used as evidence on
which to base a costly policy change in City A: after all the causal
model of the opponents would lead them to believe that the intervention
would have a negative effect in City A on either the same outcome valued
by all, or on different but equally important outcome.

Knowing about these criticisms, but still convinced that scientific
research should play a role in policy decision making, researchers have
developed a series of changes in practice to address some of those
criticisms. So, for example, a growing number of evidence-based policy
groups pre-register their analyses, carefully document the complex flows
of data files and code files that merge, clean, and analyze. These
groups also try to share as much of their data as possible and engage in
internal quality checks like blind re-analysis of results and/or
code-reviews (where as much as possible done so as to be available to
the public). Since many of the studies done by these groups involve
randomization, they tend to rely on design-based statistical tests so as
to protect their statistical inferences from false positive errors
arising from statistical technique choices.

Although those efforts have increased the utility of the policy
evaluations those groups produce by reducing the plausibility of some
criticisms, they have not addressed the concerns about the surprisingly
large result from a single study arising by chance, the concerns that a
large, positive causal effect in one context might be small, or
negative, in another context or for a different important outcome.

\hypertarget{the-promise-of-coordinated-randomized-studies}{%
\section{The promise of coordinated randomized
studies}\label{the-promise-of-coordinated-randomized-studies}}

Coordinated randomized studies aim to address the two of the remaining
criticisms above and provide some other benefits to scientific knowledge
cumulation (cites perhaps examples).

If two or more studies investigate the \(Z \rightarrow Y\) relationship,
measure both \(Z\) and \(Y\) the same way, manipulate \(Z\) in the same
way, and these activities and data collection in the same context (place
and time), then we can respond to the first set of concerns about
confusing chance events with causal effects. It would be very surprising
to see two let alone three false positives where the only reason for
differences between studies would be chance differences in the
distributions of \(Z\) and \(Y\).

If two or more studies occur in different contexts, but, as much as
possible, coordinate measurement and intervention, then we create
information to address the second set of concerns about heterogeneity of
causal effects. Of course, differences in detected causal effects
between the two studies could also arise from chance and not context.
So, adding more studies helps us distinguish between context-based
differences in effect and chance differences due to false positives.
(TODO: Elaborate on this a bit)

In these two kinds of coordinated experiments --- where effort goes into
coordinating measurement and intervention in the same or different
contexts --- another benefit arises: an overall causal effect of \(Z\)
on \(Y\) can be estimated by combining the studies, thereby increasing
the statistical precision of the estimation of this causal effect over
that possible in any single study. For example, one can treat the
multiple experiments as a single block-randomized experiment and one can
estimate an average causal effect for the units in all the
studies.\footnote{If the causal effects might vary greatly across
  blocks, say, from strongly negative to positive, the true average
  causal effect could be zero even if the individual level causal
  effects were not zero. Of course, one could also assess the
  \(Z \rightarrow Y\) causal relationship using other tools which would
  detect a strong relationship in this case.}

TODO: Either side step or talk about observational studies where
coordination has yet more benefits in helping us manage the biases
inherent in non-randomized studies. See Rosenbaum's 2021 Replication and
Evidence Factors in Observational Studies.

So, if we can address all of the criticisms that opponents of a policy
articulate using strong research transparency and integrity practices
combined with coordinated studies, should the policy makers in City A
now have an easier time making decisions than they did before these
developments?

\hypertarget{challenges-in-the-use-of-coordinated-studies-by-policy-makers}{%
\section{Challenges in the use of coordinated studies by policy
makers}\label{challenges-in-the-use-of-coordinated-studies-by-policy-makers}}

Yes. The evidence in the hands of the policy makers in City A is
certainly more useful if it arrives from the kinds of processes just
described. Yet, the criticisms and responses just described left out the
political and human costs of error on the part of the policy makers as
well as other challenges. Two conversations with policy makers (so far)
crystallized many of these points for me, which I had also observed as I
participated in the design of a coordinated experiment across 5 cities
on coaching and kindergarten readiness. So, I write this list as
questions and answers even even if this is not a record of any single
dialogue.

\textbf{Q:} How can a policy maker in City A use the results from a
coordinated study done in Cities B,\(\ldots\)?

\textbf{A:} If one of those cities is similar to City A, then the
results \textbf{for that city} would be relevant for decisions in City
A. Results in not-similar cities or cities where the intervention
differed from the possible intervention for City A would be ignored for
fear that they would be misleading in regards the possible effect in
City A. For example, in one city making transportation free would end up
saving money overall given the costs of fare collection (the technology
in that city, the use of the system, etc.) but in another city, free
public transportation would not save money for the system (even if it
could increase other tax revenues for the city). So, free public
transportation might not be a realistic policy level for City A even if
those in City A believed the causal effect of, say, free public
transportation on school absences and unemployment in City B.

\textbf{Q:} When would an overall result, calculated using all of the
cities (like a single average causal effect) be useful for policy in
City A?

\textbf{A:} If a philanthropic foundation used that overall effect to
fund this intervention very broadly (say, in any city), the implicit
causal model of the outcome and intervention in City A did not suggest
harm, and the intervention did not face political opposition, City A
would be willing to try to create a locally relevant implementation of
the intervention. (Basically, if the intervention was costless to City A
and may well help but certainly wouldn't harm the public in City A, then
policy makers would be willing to give it a try. Here the overall effect
operates to convince philanthropists at the national level, not policy
makers in City A.)

\textbf{Q:} What about state-of-the art extrapolation from the overall
results of the coordinated experiment to City A using covariates
measured in both places? Would that help City A use the results?

\textbf{A:} Conditional on the intervention being politically and
practically plausible and covariates measured the same way, with the
same meaning in both place, then if such tools confirmed prior beliefs,
then they would not be rejected. Also, differences in implementation
context and policy context appear difficult to weight as compared to
differences in age distribution as measured by the US Census.

However, description of variation in effects across cities might be used
in a kind of informal version of the extrapolation/transportation
approaches. That is, there is interest in how the effects vary according
to certain covariates even if there is distrust of weighted combinations
of that variation using those same covariates. (This suggests that one
could present \textbf{both} approaches and perhaps showing the weighted
combination/prediction would help interpretation of the raw variation
information.)

\textbf{Q:} What would it mean to \textbf{use} a result in City A?

\textbf{A:} A result inspires a discussion about how to adapt the
approach in the other places to the local context. City A knows that it
will implement and measure the intervention and outcome differently than
other cities and that the context (history, institutions, demography,
geography) of City A differs from other cites as well. A causal effect
from a coordinated trial (probably from the cities like City A) inspires
a process of improvisation local to City A rather than, say, a franchise
ensuring that the policy in City A implemented the same way and outcomes
are measured the same was as they are in the other cities: there is
little authority or desire for such standardization to the extent that
the aim for City A is to most improve lives in City A.

\textbf{Q:} What else might help City A use evidence from a coordinated
study in its evidence-based policy making?

\textbf{A:} Auxiliary studies that focus on the many questions that
arise about differences in measurement and implementation would help.
For example, if we measure verbal ability in Kindergarten using Test A,
but the coordinated study used Test B in 3rd grade, are there results
that show that children who took Test B in 3rd grade would have similar
rankings in scores on Test A in Kindergarten? Is there a small lab study
that would illuminate this question? What about the effect of
recruitment to meetings by people of same or different race? Are there
any studies about responses to door-to-door canvassing where people
knocking on the doors are the same or different race as the people
answering the door?

\textbf{Q:} What might prevent use of a result?

\textbf{A:} City A would need to be convinced that the kinds of people
that the intervention aims to help were not harmed and in fact benefited
from the intervention whether or not people benefited on average. For
example, some jurisdictions make racial equity a criteria for policy
interventions. A finding that, on average, people benefited but people
of color did not benefit, would prevent the use of the result. Above and
beyond the subgroup specific effects, City A might also need to explain
why the intervention would cause no harm when building a coalition to
support the new policy.

\medskip

The preceding fake conversation reveals that concerns about causal
models/mechanisms for interventions, concerns about distributional
consequences, and concerns about local implementation possibilities
could all prevent the use of a given piece of evidence by policy makers.

Policy makers have implicit (or explicit) causal models relating
interventions to outcomes. For example, Figure \ref{fig:theory2} shows a
stylized causal model that a policy maker might have of kindergarten
readiness in their own city (thus no \(\boxed{S}\) below). The coaching
intervention might improve kindergarten readiness, but so might
addressing Family SES, Neighborhood Poverty, Teacher Quality, or
Classroom sizes.

\begin{figure}[H]
\centering
\begin{tikzcd}[every arrow/.append style=-latex]
{\text{Family SES}}  \arrow[from=1-1, to=1-2] \arrow[from=1-1, to=2-1] & {\text{Kindergarten Readiness Verbal Skills}} \\
{\text{Home Language Environment Quality}}   \arrow[from=2-1, to=1-2] \\
{\text{Neighborhood Poverty}}  \arrow[from=3-1, to=1-2] \\
{\text{Pre-School Classroom Size}}       \arrow[from=4-1, to=1-2] \\
{\text{Pre-School Teacher Quality}} \arrow[from=5-1, to=1-2] \\
   \end{tikzcd}
\caption{A theory of educational outcomes at the level of concepts and
relationships.}\label{fig:theory2}
\end{figure}

Before hearing about the coordinated coaching study, the policy makers
in City A might not have had the ``Home Language Environment Quality''
as a part of their causal model. But upon hearing about the study, they
might be willing to elaborate their causal model to add new elements to
it. For example, they might be willing to believe that the intervention
caused the outcome in the other cities given the benefits of
transparency and coordination mentioned earlier.

Once the new element is added to the causal model, questions about
implementation and distribution arise. Should they implement the
coaching approach that appears promising from a study that ensured that
all of the coaching interventions were done the same way across multiple
cities? City A might do so only if they could imagine a plausible mode
of implementing that coaching intervention (where ``plausible'' means
not creating political conflict, and not costing more than intervening
in any of the other parts of the causal model to achieve the same result
on this or other important outcomes). For example, if home visiting
nurses were the coaches in the cities in the coordinated study, and if
City A had an active home visiting nurse program for low income
families, the implementation of the program in City A might appear
easier and plausible. However, if City A had decided long ago to not use
home visiting nurses, and provided similar services in a different way,
then City A might find the results of the coordinate study less
relevant.

Imagine the City A did have a home visiting program that aimed to
support low income families --- in City A, imagine that these people
were mostly agricultural workers. But the coordinated study did not
report results for such people. City A would then wonder about whether
the positive result from the coordinated study would be useful for its
population. Further, imagine that results from the coordinated
experiment were broken down within each city by income and type of
employment. The decision makers in City A would ask whether coaching
from nurses in City A would have the same kinds of effects as they would
in the other Cities. Would living in apartment buildings change the
effect? Or would home language matter? Would family structure matter?
The discussion would turn to causal mechanisms.

So, what is the benefit of the coordinated experiment for the policy
makers in City A? It appears to be mostly to encourage them to add an
element to their causal model if the study is compelling about causal
effects in the other cities. It could also encourage them to build their
own intervention inspired by the results of the coordinated experiment
if the intervention in the coordinated study was similar to something
that City A could realistically do and decision makers in City A
believed that the context of City A would not prevent the causal process
from operating there.

\hypertarget{discussion-and-next-steps-in-evidence-based-policy-making}{%
\section{Discussion and next steps in evidence-based policy
making}\label{discussion-and-next-steps-in-evidence-based-policy-making}}

Coordinated studies and transparency practices help researchers add and
remove paths from their shared causal model. In turn, a scientific
consensus about the nodes and paths of a model can help policy makers
elaborate their own causal models or ``theories of change''.

What else do policy makers need? Focused studies on key elements of the
causal model could help. They need not be coordinated, but should
explicitly target a causal pathway of interest to a policy-maker or
perhaps inform the policy-maker about the consequences of differences in
measurement. These are the kinds of studies that academics may already
be doing or find easy to do in their labs. The answers to the
theoretical questions about causal pathways and measurement would guide
decision making even if the labs themselves did not represent the
populations where the intervention would be rolled out.

Coordinated studies as they currently exist have another strength for
the policy makers for City A: they enable studies of variation in causal
effects because the measurement and intervention strategies are similar
across contexts. To the extent that the contexts differ in ways that
enable policy makers to extrapolate to their own context, then perhaps
they could inform decisions. In this way, perhaps the extrapolation and
transportation-of-effects approaches could be useful if presented as a
kind of summary of existing variation.

What else might make coordinated studies yet more useful? The planning
and design of coordinated studies involves making implicit causal models
explicit, and shared causal models enables the creation of multi-study
research agendas --- each targeting different causal pathways. One
approach to ensure that many causal models are articulated and discussed
to avoid coordinated studies reporting irrelevant information would
involve, say, 5 cities where the study would be implemented plus 5 other
cities where the study would not be implemented, but which would play a
role in the study design. This approach might make the measurement and
intervention more relevant to more cities without having to pay for 10
studies. (TODO: Elaborate a bit on this idea of including non-field
sites in the design if not also analysis process of a coordinated study.
Especially think about the benefits of this idea from perspective of
costs of error: engaging with questions like ``what could go wrong?''.)

Another approach would involve the creation of collective learning
agendas that would be shared across both academics and jurisdictions.
Here, the coordination would not be about any given study, but would
mostly focus on the putative/proposed/a priori plausible causal models.
Efforts in this regard are underway in the area of public transportation
--- where a monthly call of many transportation agencies across the
country is occurring and where academics interested in transportation
are invited to participate, and where results from randomized trials in
individual cities are presented and discussed. This Interjurisdictional
Transit Equity Research Collaborative (ITERC) model aims to develop a
community of practice among maybe 50 cities and 10 or so academics in
regards public transportation and allows a shared sense of different,
mostly implicit, causal models and also approaches to the operation of
public transportation systems.

Although there are more interviews to collect in a more formal way to
learn about how a broader array of policy makers might use the results
of coordinated experiments, the observations and interviews done so far
make it clear that the benefits of coordinated experiments to social
scientists are not the same as the benefits to policy makers. However, I
suspect that, at least for a subset of coordinated experiments, we
researchers can make them more policy relevant if we include
policy-makers in the planning, design, and analysis processes.

\printbibliography

\end{document}
