---
title: Generalization, Induction, Agency \n How to speed social science and policy learning from scattered, idiosyncratic and context-bound research designs 
author: Jake Bowers and James Kuklinski
date: 10 July 2020
---

# Abstract

We provide guidance for those making research design choices when the aim is to learn from one study and apply those learnings to another study, another place, or another moment in time. While we cannot solve Hume's problem of induction, we as a community of scholars and practitioners can learn together faster and better if we follow a few reasonable practices and if we understand our joint endeavor as focused on learning about theory or explanations and we avoid the mistake of thinking that any single study in and of itself can guide future decisions without a human in the loop.

# Only Humans Generalize. Cricisisms of research designs on the basis of generalizability are not valid or relevant.

No research design can generalize in and of itself. Human beings as social scientists, policy-makers, generalize. When we talk about a "generalizable finding" we are really saying that "this finding makes it easy for us to generalize". What are we doing when we generalize? Why do we care to generalize?

Scientists want to improve their explanations of the world and policy makers want to improve their decisions and policies that change the world. The "evidence-based public policy" movement (including the US govt's Foundations for Evidence-Based Policymaking Act of 2018) 

Our contribution in this piece is synthetic. The problem of induction has was more forefully articulated by David Hume, the idea that novelty and difference speeds learning more than repetition can be found elsewhere (recently formalized in the context of hypothesis testing by Paul Rosenbaum, but also implicit in the statistics of multiple hypothesis testing in genearl), the idea that the point of research design is to learn about theory is to widespread as to be difficult to find a canonical cite. We hope that by collecting and connecting these pieces of insight we can help current scientists and policy makers speed learning in moments when collective learning appears more and more urgent.

We begin by briefly explaining what we mean when we say "No research design can generalize". Then we discuss the promise the replication and coordination holds, pointing to recent success stories such as the EGAP Metaketa initiative of coordinated and co-designed field experiments <https://egap.org/our-work/the-metaketa-initiative/> and the COVID clinical trial collaboration platform <https://covidcp.org/>. We point out that repeatedly finding the same result helps exclude some kinds of alternative explanations while failing to exclude others and we propose a modification on the idea of coodinated replication that we call "coordinated difference replication". Such an approach, in turn, requires more work up front by scientific and policy communities to be more specific about theoretical expectations and explanations. We argue that knowledge cumulation and speedy learning require both work on theory as well as on research design so that we have a "theory-and-evidence-based public policy".

# All social science research is contextual and idiosyncratic

[JB: Insert previous text about this]


# Reasonable practices that ease generalization

A good research design helps us rule out alternative explanations for the results. When a policy maker is looking for past studies that might guide decision making for a current challenge, what studies should she choose to pay attention to? The ones from Harvard? Only the RCTs? Only those involving a random probability sample of some relevant population of units? Only those involving large samples? Only those studies where the results are in line with the existing beliefs of the policy maker?

## How does replication ease generalization? What kind of replication should speed learning?

Two current main types of replication: (1) Sequential Replication: A sequence of studies that differ in time but that aim to precisely repeat the same research design (for example, one city fields an experiment to learn to improve messaging related to social distance during a pandemic. After the results are in from that first city, a second city tries to field the exact same research design.); (2) Multiple coordinated studies: For example, multiple cities agree to harmonize and coordinate the details of the social messaging experiment from the beginning. The experiment is fielded more or less simultaneously by all cities without knowing the results from any single city.

These two approaches to replication promise to speed learning. How might learning increase in speed via such replication? In some ways, it should not improve very much, especially if the same result emerges from each study --- after if the research design is the same in each place, then we only increase our learning if the results differ across places. However, these modes of replication do in fact speed learning because they help the scientific and policy community exclude contextual and methodological criticisms. Any single study can be discounted because either (a) the context is too idiosyncratic ("You are just telling me that people trust government in Scandinavia, you are not really teaching me (i) a new understanding of social norms or (ii) that social norm related messages are broadly better (or worse) than self-interest focused messages.") or (b) something about the methods of data collection and analysis raises other criticisms of the study itself. That is, in a world in which social science research can be so easily discounted as (i) overly contextual, (ii) methodologically flawed, or (iii) actively ideological (this is the criticism of "evidence-based policy" as really just the use of science as a thin veneer for ideological arguments or "policy-based evidence") coordination and replication in general can help scientific and policy communities side-step and/or confront and discard such criticisms and doubts.

However, those approaches do not in and of themselves help with discarding the kinds of alternative explanations that are not based on context, method, or general skepticism and ideological difference. Any of those explanations not well excluded or argued against in the first study (or the overall plan) will not be well excluded or argued against in the other studies. For example, coordinated studies that show that smokers to non-smokers differ in cancer rates do help us in this way, but they do not help us exclude the counter argument that smokers choose to smoke because of a genetic predisposition that *both* encourages smoking *and* cancer. To get past that criticism, we would need a different kind of coordination across studies --- coordination to differ.

What about a third model where multiple cities agree to coordinate experiments that differ in specific and pre-determined ways? How would we identify the ways in which experiments should differ? Should the experiments also coordinate --- say, have one common arm but then add distinctive arms, each meant to assess another implication of the motivating theory?

# Conclusion/Discussion

Elaborate theories (and "theory-and-evidence-based public policy")

Diverse replication.

The continuing value of randomized assignment and random samples.

# References





